import React, { useState, useEffect, useRef } from "react";
import "./TalkingScreen.css";
import { HiOutlineMicrophone } from "react-icons/hi2";
import { FaRegStopCircle, FaRegPlayCircle } from "react-icons/fa";
//-----------------------------------‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ wakeword-----------------------------------------
import { usePorcupine } from "@picovoice/porcupine-react";
import HelloAvisKeywordModel from "../Hello_avis";
import ThankYouAvisKeywordModel from "../Thank-you-Avis";
import modelParams from "../porcupine_params";

function TalkingScreen({ keywordDetection, onClose }) {
  const [scale, setScale] = useState(1);
  const [hasSound, setHasSound] = useState(false);
  const [showStopIcon, setShowStopIcon] = useState(true);
  const audioChunksRef = useRef([]);
  const mediaRecorderRef = useRef(null);
  const [isRecording, setIsRecording] = useState(false);
  const silenceTimeoutRef = useRef(null);
  const maxRecordTimeoutRef = useRef(null);
  const { isLoaded, init, start, stop } = usePorcupine();
  const wsRef = useRef(null); // ‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏® websocket
  const [isSpeaking, setIsSpeaking] = useState(false);
  const playTimeoutRef = useRef(null);
  const [isListening, setIsListening] = useState(true);  // ‡πÄ‡∏û‡∏¥‡πà‡∏° state isListening
  const HelloavisRef = useRef(false); // ‡πÉ‡∏ä‡πâ useRef ‡πÅ‡∏ó‡∏ô useState 
  const [isPaused, setIsPaused] = useState(false);  // ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß
  const [isPlayingResponse, setIsPlayingResponse] = useState(true);
  const lastKeywordRef = useRef(null); // ‡πÉ‡∏ä‡πâ ref ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤ keyword ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
  const [isProcessing, setIsProcessing] = useState(false);


  useEffect(() => {
    if (isPlayingResponse === false) { // ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
      console.log("‚úÖ ‡πÄ‡∏õ‡∏¥‡∏î‡∏£‡∏∞‡∏ö‡∏ö‡∏ü‡∏±‡∏á wake word ‡πÉ‡∏´‡∏°‡πà...");
      setIsPaused(false); // ‡πÄ‡∏õ‡∏¥‡∏î wake word ‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á
      setIsListening(true);
      start(); // ‡πÄ‡∏õ‡∏¥‡∏î‡∏£‡∏∞‡∏ö‡∏ö‡∏ü‡∏±‡∏á wake word
    }
  }, [isPlayingResponse]);

  useEffect(() => {
    if (!isLoaded) {
      init("pGO4BAYiyE5xOsIbk5ybzw38zI1oTal4m5vqHkR+XGfEiNwpL8IGLw==", [
        { base64: HelloAvisKeywordModel, label: "Hello Avis" },
        { base64: ThankYouAvisKeywordModel, label: "Thank you Avis" },
      ], { base64: modelParams }).then(() => start());
    }
  }, [isLoaded, init, start]);

  useEffect(() => {
    
    if (!keywordDetection || !isListening) return;
    if (lastKeywordRef.current === keywordDetection.label) return;
    console.log("üîç Detected keyword:", keywordDetection.label);
    lastKeywordRef.current = keywordDetection.label; // ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ñ‡πà‡∏≤‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î
    console.log(lastKeywordRef.current)
    
    if (lastKeywordRef.current === "Hello Avis") {
      const text = "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö ‡∏≠‡∏¢‡∏≤‡∏Å‡∏ó‡∏£‡∏≤‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏≠‡∏∞‡πÑ‡∏£‡∏Ñ‡∏£‡∏±‡∏ö"
      const speechSynthesisUtterance = new SpeechSynthesisUtterance(text);
      speechSynthesisUtterance.rate = 0.8;    
      speechSynthesisUtterance.volume = 0.9; 
      speechSynthesisUtterance.pitch = 1.2;  
      const voices = speechSynthesis.getVoices();
      speechSynthesisUtterance.voice = voices.find(voice => voice.name.includes("Google ‡πÑ‡∏ó‡∏¢"));
      speechSynthesis.speak(speechSynthesisUtterance);
      HelloavisRef.current = true;
      setIsPaused(true);
      setTimeout(startRecording, 2500);
    } else if (lastKeywordRef.current === "Thank you Avis") {
      const text = "‡∏Ç‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ Avis ‡∏Ñ‡∏£‡∏±‡∏ö"
      const speechSynthesisUtterance = new SpeechSynthesisUtterance(text);
      speechSynthesisUtterance.rate = 0.8;    
      speechSynthesisUtterance.volume = 0.9; 
      speechSynthesisUtterance.pitch = 1.2;   
      const voices = speechSynthesis.getVoices();
      speechSynthesisUtterance.voice = voices.find(voice => voice.name.includes("Google ‡πÑ‡∏ó‡∏¢"));

      speechSynthesis.speak(speechSynthesisUtterance);
      console.log("üëã Goodbye!");
      lastKeywordRef.current = null;
      wsRef.current?.close();
      stop(); 
      stopRecording()
      if (typeof onClose === "function") {
        onClose();  
      }
    }
  }, [keywordDetection, isListening]);


  const startRecording = async () => {
    if (mediaRecorderRef.current?.state === "recording" || !HelloavisRef.current) return;

    console.log("üéôÔ∏è ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á...");
    setShowStopIcon(false);

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const recorder = new MediaRecorder(stream);
      audioChunksRef.current = []; // ‡∏£‡∏µ‡πÄ‡∏ã‡πá‡∏ï‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡πà‡∏≤

      recorder.ondataavailable = (e) => {
        if (e.data.size > 0) {
          console.log("üîπ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á chunk...");
          audioChunksRef.current.push(e.data);
        }
      };

      recorder.onstop = async () => {
        console.log("üõë ‡∏´‡∏¢‡∏∏‡∏î‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á");
        if (audioChunksRef.current.length === 0) return;

        const audioBlob = new Blob(audioChunksRef.current, { type: "audio/wav" });
        console.log("üéµ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á...");
        setIsProcessing(true);  // ‡πÄ‡∏õ‡∏¥‡∏î Loader
        sendAudioToAPI(audioBlob);
      };

      mediaRecorderRef.current = recorder;
      recorder.start();
      setIsRecording(true);
      maxRecordTimeoutRef.current = setTimeout(() => recorder.stop(), 6500);
    } catch (error) {
      console.error("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏î‡πâ:", error);
    }
  };


  // üõë ‡∏´‡∏¢‡∏∏‡∏î‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á
  const stopRecording = () => {
    if (!isRecording || !mediaRecorderRef.current) return;
    console.log("üõë ‡∏´‡∏¢‡∏∏‡∏î‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏™‡∏µ‡∏¢‡∏á");
    mediaRecorderRef.current.stop();
    setIsRecording(false);
    setShowStopIcon(true);
  };

  // // üì® ‡∏™‡πà‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏õ API
  // const sendAudioToAPI = async (audioBlob) => {
  //   try {
  //     console.log("üì® ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏õ API...");
      
  //     // üõë ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà
  //     if (audioBlob.size === 0) {
  //       console.warn("‚ö†Ô∏è ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡πÑ‡∏õ API");
  //       await playVoice("‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏û‡∏π‡∏î‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Ñ‡∏£‡∏±‡∏ö");
  //       return;
  //     }
  
  //     // ‡πÅ‡∏õ‡∏•‡∏á Blob ‡πÄ‡∏õ‡πá‡∏ô ArrayBuffer
  //     const arrayBuffer = await new Promise((resolve, reject) => {
  //       const reader = new FileReader();
  //       reader.onloadend = () => resolve(reader.result);
  //       reader.onerror = reject;
  //       reader.readAsArrayBuffer(audioBlob);
  //     });
  
  //     // ‡πÅ‡∏õ‡∏•‡∏á ArrayBuffer ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á (PCM float32)
  //     const audioContext = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
  //     const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
  //     const channelData = audioBuffer.getChannelData(0);
  //     const audioArray = Array.from(channelData);
  
  //     // üì® ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏õ API
  //     const response = await fetch("https://142f-202-44-40-186.ngrok-free.app/transcription", {
  //       method: "POST",
  //       headers: { "Content-Type": "application/json" },
  //       body: JSON.stringify({ audioData: audioArray }),
  //     });
  
  //     // üîç Debug API Response
  //     const responseText = await response.text();
  //     console.log("üîç API Response:", responseText);
  
  //     if (response.ok) {
  //       const result = JSON.parse(responseText);
  //       const text = result.output.results.map((item) => item.transcript).join(" ");
  //       console.log("üìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏î‡πâ:", text);
  //       sendTextToLocalServer(text);
  //     } else {
  //       console.error("‚ùå API Error:", response.status, response.statusText);
  //       sendTextToLocalServer(""); // ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡∏ñ‡πâ‡∏≤‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ
  //     }
  //   } catch (error) {
  //     console.error("‚ùå Error processing audio:", error);
  //   }
  // };
  // üì® ‡∏™‡πà‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏õ API


const sendAudioToAPI = async (audioBlob) => {
  try {
      console.log("üì® ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡πà‡∏á‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏õ API...");
      if (audioBlob.size === 0) {
        console.warn("‚ö†Ô∏è ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ß‡πà‡∏≤‡∏á‡πÄ‡∏õ‡∏•‡πà‡∏≤ ‡πÑ‡∏°‡πà‡∏™‡πà‡∏á‡πÑ‡∏õ API");
        await playVoice("‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏û‡∏π‡∏î‡πÉ‡∏´‡∏°‡πà‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏Ñ‡∏£‡∏±‡∏ö");
        return;
      }
      const formData = new FormData();
      formData.append("audioData", audioBlob, "audio.wav");

      const response = await fetch("https://api.gowajee.ai/v1/speech-to-text/cosmos/transcribe", {
          method: "POST",
          headers: { "x-api-key": "gwj_live_407fb85990e246ce9462a3b71357a2c1_bo23x" },
          body: formData,
      });

      if (response.ok) {
          const result = await response.json();
          const text = result.output.results.map((item) => item.transcript).join(" ");
          console.log("üìù ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏≠‡∏î‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡πÑ‡∏î‡πâ:", text);
          sendTextToLocalServer(text);
      }else{
        const text = " "
        sendTextToLocalServer(text);
      }
  } catch (error) {
      console.error("‚ùå Error processing audio:", error);
  }
};
  


const sendTextToLocalServer = async (text) => {
  try {
    console.log("üì© ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏õ Local Server:", text);

    const response = await fetch('https://d51c-202-44-40-196.ngrok-free.app/call_llm', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({ text: text })
    });
    

    if (response.ok) {
      const result = await response.json();
      console.log("üîä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å AI...");
      await playVoice(result.responses);
    } else {
      console.log("‚ùå API Response Error:", response.status);
      await playVoice("‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ñ‡∏£‡∏±‡∏ö");
    }
  } catch (error) {
    console.error("‚ùå Error sending text:", error);
  }
};


  const playVoice = async (text) => {
    try {
      if (!HelloavisRef.current) return;
      setIsProcessing(false); 
      setShowStopIcon(false);
      stop();
      setIsPaused(true);
      lastKeywordRef.current = null;
      console.log(lastKeywordRef.current)
      console.log("üîä ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÄ‡∏•‡πà‡∏ô‡πÄ‡∏™‡∏µ‡∏¢‡∏á:", text);
      const speechSynthesisUtterance = new SpeechSynthesisUtterance(text);
      speechSynthesisUtterance.rate = 0.8;   
      speechSynthesisUtterance.volume = 0.9;  
      speechSynthesisUtterance.pitch = 1.0;   
      const voices = speechSynthesis.getVoices();
      speechSynthesisUtterance.voice = voices.find(voice => voice.name.includes("Google ‡πÑ‡∏ó‡∏¢"));
      speechSynthesis.speak(speechSynthesisUtterance);

      speechSynthesisUtterance.onend = () => {
        console.log("üîÑ ‡πÄ‡∏õ‡∏¥‡∏î wake word ‡πÉ‡∏´‡∏°‡πà...");
        setIsPlayingResponse(false)
        setIsPaused(false);
        HelloavisRef.current = false;
        setTimeout(() => {
          start();
        }, 1500);  
      };
    } catch (error) {
      console.error("‚ùå Error playing voice:", error);
    }
  };





  //-------------------------------------------
  //-----------------------------------‡πÇ‡∏Ñ‡πâ‡∏î process ‡πÄ‡∏™‡∏µ‡∏¢‡∏á-------------------------------------
  useEffect(() => {
    const setupAudio = async () => {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const analyser = audioContext.createAnalyser();
        const microphone = audioContext.createMediaStreamSource(stream);
        const processor = audioContext.createScriptProcessor(256, 1, 1);

        analyser.smoothingTimeConstant = 0.8;
        analyser.fftSize = 1024;

        microphone.connect(analyser);
        analyser.connect(processor);
        processor.connect(audioContext.destination);

        processor.onaudioprocess = function () {
          const array = new Uint8Array(analyser.frequencyBinCount);
          analyser.getByteFrequencyData(array);
          const average = array.reduce((sum, value) => sum + value, 0) / array.length;
          const volume = Math.min(average / 100, 1);

          if (isRecording) {
            setScale(1 + volume);
            setHasSound(volume > 0.18);

            // if (volume > 0.18) {
            //   clearTimeout(silenceTimeoutRef.current);
            //   silenceTimeoutRef.current = setTimeout(() => {
            //     stopRecording();
            //   }, 3000);
            // }
          }
        };

        return stream;
      } catch (error) {
        console.error("‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏™‡∏µ‡∏¢‡∏á:", error);
      }
    };

    setupAudio();
  }, [isRecording]);


  return (
    <div style={{ display: 'flex', flexDirection: 'column', alignItems: 'center', justifyContent: 'center', height: '100vh' }}>
      {isProcessing ? (
        <div className="loader"></div>  // ‡πÅ‡∏™‡∏î‡∏á Loader ‡∏Ç‡∏ì‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
      ) : (
        <div
          className="circle"
          style={{
            transform: `scale(${isRecording ? scale : 1})`,
            backgroundColor: isRecording ? (hasSound ? "white" : "gray") : "gray",
          }}
        >
          <HiOutlineMicrophone size={120} color="black" />
        </div>
      )}

      {/* Recording / Stop icons */}
      <div style={{ position: "absolute", bottom: "50px", display: "flex", justifyContent: "flex-end", width: "100%", paddingRight: "355px" }}>
        {isProcessing ? (
          <FaRegStopCircle size={120} style={{ cursor: "pointer", color: "gray" }} />
        ) : (
          <FaRegPlayCircle
            onClick={startRecording}
            size={120}
            style={{ cursor: "pointer", color: "green" }}
          />
        )}
      </div>
    </div>

  );
}
export default TalkingScreen;